{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n",
    "import datetime\n",
    "from datetime import datetime,timedelta\n",
    "import time\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(name):\n",
    "    data=pd.read_csv(\"path/\"+name+\".csv\")\n",
    "    return data\n",
    "\n",
    "def read_excel(name,sheet_name):\n",
    "    data=pd.read_excel(\"path/\"+name+\".xlsx\",sheet_name=sheet_name)\n",
    "    return data\n",
    "\n",
    "def save(data,name):\n",
    "    data.to_csv(\"path/data/\"+name+\".csv\")\n",
    "    \n",
    "def getlist(df,col):\n",
    "    acc_list=list(df[col].drop_duplicates())\n",
    "    return acc_list\n",
    "\n",
    "def loc_one(df,col,item):\n",
    "    loc_df=df.loc[df[col]==item]\n",
    "    return loc_df\n",
    "\n",
    "def loc_list(df,col,acc_list):\n",
    "    loc_df=df.loc[df[col].isin(acc_list)]\n",
    "    return loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data by segment\n",
    "data=read_csv('filtered_data6')\n",
    "segment_info=read_csv('teamid with names')\n",
    "smb_list=getlist(loc_one(segment_info,'segment','SMB'),'teamid')\n",
    "mm_list=getlist(loc_one(segment_info,'segment','MM'),'teamid')\n",
    "ent_list=getlist(loc_one(segment_info,'segment','ENT'),'teamid')\n",
    "\n",
    "seg_list=smb_list\n",
    "fil_data=loc_list(data,'teamid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.555555555555557\n"
     ]
    }
   ],
   "source": [
    "### ONE find the bottom 5% view level for all users\n",
    "fil_data=read_csv('filtered_data6')\n",
    "\n",
    "\n",
    "data=fil_data\n",
    "name_dict=dict(zip(data.teamid,data.company\n",
    "acc_list=getlist(data,'team_id')\n",
    "\n",
    "#1 combine all files into 1 table & filter with current acc list\n",
    "mypath='path...'\n",
    "file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "user_data=pd.DataFrame()\n",
    "for file in file_names:\n",
    "    df=pd.read_csv(mypath+'/'+file)\n",
    "    user_data=user_data.append(df)\n",
    "#save(user_data.'combined_df')\n",
    "#user_data=read_csv('combined_df')\n",
    "all_data=loc_list(user_data,'team_id',acc_list)\n",
    "all_data=loc_one(all_data,'team_report_type','function2')\n",
    "\n",
    "#2 get monthly data for each user\n",
    "all_data['date']=pd.to_datetime(all_data['date'],format='%Y-%m-%d %H:%M:%S')\n",
    "month_view=all_data.groupby(['userid',pd.Grouper(key='date',freq='M')])['value'].sum().reset_index()\n",
    "user_avg_view=month_view.groupby(['userid'])['value'].mean().reset_index()\n",
    "usage=list(user_avg_view['value'])\n",
    "\n",
    "#3 get 5% quantile\n",
    "bottom_quantiles=user_avg_view.quantile([0.05,0.1,0.25,0.3])\n",
    "bottom_25=user_avg_view.quantile([0.25]).reset_index()\n",
    "low_view=bottom_25['value'][0]\n",
    "print(low_view)\n",
    "\n",
    "### TW0 filter those with accounts with consistent low use\n",
    "#1 count num of user in each account\n",
    "num_of_users=all_data.groupby('teamid')['userid'].nunique()\n",
    "user_num_dict=num_of_users.to_dict()\n",
    "\n",
    "#2 filter the accounts using benchmark set by number of users and 25%quantile \n",
    "problems=[]\n",
    "consistent_low_list=[]\n",
    "avg_low_list=[]\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view=list(acc_df['function2'])\n",
    "    view.remove(max(view))\n",
    "    try:\n",
    "        low_use=user_num_dict[acc]*low_view\n",
    "        avg_view=sum(view)/len(view)\n",
    "        high_view = [i for i in view if i > low_use]\n",
    "        if high_view == []:\n",
    "            consistent_low_list.append(acc)\n",
    "            #consistent_low_df=consistent_low_df.append(com_df)\n",
    "        if avg_view <= low_use:\n",
    "            avg_low_list.append(acc)\n",
    "            #avg_low_df=low_use_df.append(com_df)\n",
    "    except:\n",
    "        problems.append(acc)\n",
    "problem_df=loc_list(data,'teamid',problems)\n",
    "consis_low_df=loc_list(data,'teamid',consistent_low_list)\n",
    "avg_low_df=loc_list(data,'teamid',avg_low_list)\n",
    "#save(problem_df,'new_customers')\n",
    "#save(low_use_df,'low_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter sudden drop - initial\n",
    "\n",
    "drop_pct=0.9\n",
    "\n",
    "#fil_data=read_csv('filtered_data')\n",
    "data=fil_data\n",
    "name_dict=dict(zip(data.teamid,data.company))\n",
    "acc_list=getlist(data,'teamid')\n",
    "\n",
    "#1 for those accs with >= 4 months, identify drop based on first 3 months of data, put them into dicts\n",
    "drop_mon_dict=dict()\n",
    "drop_amount_dict=dict()\n",
    "rise_amount_dict=dict()\n",
    "total_month_dict=dict()\n",
    "first_drop_month=dict()\n",
    "first_drop_date=dict()\n",
    "zero_use=[]\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view_list=list(acc_df['function2'])\n",
    "    date_list=list(acc_df['date'])\n",
    "    initial_view= sum(view_list[0:3])/3\n",
    "    #initial_view_list= view_list[0:3]\n",
    "    #low_use=user_dict[com]*low_view\n",
    "    #low_initial_view=[i for i in initial_view_list if i <= low_use]  \n",
    "    #if low_initial_view == []:\n",
    "    if initial_view != 0:\n",
    "        num_of_drop=0\n",
    "        total_months=len(view_list)\n",
    "        month_of_drop=[]\n",
    "        date_of_drop=[]\n",
    "        for i in range(len(view_list)-3):\n",
    "            month_view=view_list[i+3]\n",
    "            date=date_list[i+3]\n",
    "            change = (initial_view-month_view)/initial_view\n",
    "            if change >= drop_pct:\n",
    "                num_of_drop+=1\n",
    "                month_of_drop.append(i+4)\n",
    "                date_of_drop.append(date)\n",
    "        num_of_rise=total_months-num_of_drop-3\n",
    "        total_month_dict[acc]=total_months\n",
    "        drop_amount_dict[acc]=num_of_drop\n",
    "        rise_amount_dict[acc]=num_of_rise\n",
    "        try:\n",
    "            first_drop_date[acc]=date_of_drop[0]\n",
    "        except:\n",
    "            first_drop_date[acc]=np.nan\n",
    "        try:\n",
    "            first_drop_month[acc]=month_of_drop[0]\n",
    "        except:\n",
    "            first_drop_month[acc]=np.nan\n",
    "        \n",
    "    else:\n",
    "        zero_use.append(acc)\n",
    "\n",
    "  \n",
    " #2 covert dicts to dataframe\n",
    "drop_df = pd.DataFrame(list(total_month_dict.items()),columns = ['team_id','total_months'])\n",
    "drop_mon=pd.DataFrame(list(drop_amount_dict.items()),columns = ['team_id','drop_mon'])\n",
    "rise_mon=pd.DataFrame(list(rise_amount_dict.items()),columns = ['team_id','rise_mon'])\n",
    "first_drop=pd.DataFrame(list(first_drop_month.items()),columns = ['team_id','first_drop_mon'])\n",
    "first_date=pd.DataFrame(list(first_drop_date.items()),columns = ['team_id','first_drop_date'])\n",
    "drop_df=pd.merge(drop_df,drop_mon,on='team_id')\n",
    "drop_df=pd.merge(drop_df,rise_mon,on='team_id')\n",
    "drop_df=pd.merge(drop_df,first_drop,on='team_id')\n",
    "drop_df=pd.merge(drop_df,first_date,on='team_id')\n",
    "\n",
    "name_df=pd.DataFrame(list(name_dict.items()),columns = ['team_id','team_name'])\n",
    "drop_df=pd.merge(drop_df,name_df,on='team_id')\n",
    "drop_df=drop_df.replace('', 0) \n",
    "\n",
    "\n",
    "#3 filter those with more drop than rise, rising months of 1 or less\n",
    "drop_df['first_drop_mon'] = drop_df['first_drop_mon'].replace({0:np.nan})\n",
    "drop_df['after_rise_mon']=drop_df['rise_mon']-(drop_df['first_drop_mon']-4)\n",
    "\n",
    "sudden_drop_info=drop_df.loc[drop_df['drop_mon'] > drop_df['after_rise_mon']]\n",
    "sudden_drop_info=sudden_drop_info.loc[sudden_drop_info['after_rise_mon']<=1]\n",
    "\n",
    "drop_com_list=getlist(sudden_drop_info,'team_id')\n",
    "sudden_drop_df=loc_list(data,'teamid',drop_com_list)\n",
    "#save(sudden_drop_df,'sudden_drop_90_overlapped')\n",
    "#save(drop_df,'drop_info_90')\n",
    "\n",
    "\n",
    "#4 get common accounts from drop_df and low_use_df\n",
    "common_acc=list(set(drop_com_list)&set(consistent_low_list))\n",
    "\n",
    "#5 for the sudden drop accounts,filter out those with sudden drop in first 3 months as they are fluctuating accounts\n",
    "data=sudden_drop_df\n",
    "drop_three_list=list()\n",
    "for com in drop_com_list:\n",
    "#for a in range(1):\n",
    "    #com='team5da5fcf33fa784.09589832'\n",
    "    com_df=loc_one(data,'teamid',com)\n",
    "    view_list=list(com_df['function2'])\n",
    "    initial_view=view_list[0]\n",
    "    if initial_view != 0:\n",
    "        for i in range(2):\n",
    "            month_view=view_list[i+1]\n",
    "            change=(initial_view-month_view)/initial_view\n",
    "            if change >= drop_pct:\n",
    "                drop_three_list.append(com)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        initial_view=view_list[1]\n",
    "        if initial_view != 0:\n",
    "            third_view=view_list[2]\n",
    "            change=(initial_view-third_view)/initial_view\n",
    "            if change >= drop_pct:\n",
    "                drop_three_list.append(com)\n",
    "            \n",
    "drop_three_df=loc_list(data,'teamid',drop_three_list)\n",
    "\n",
    "#6 remove the accs with sudden drop in first 3 months as they are fluctuating accounts from the sudden drop \n",
    "non_three_list=list(set(drop_com_list)-set(drop_three_list))\n",
    "sudden_drop_df=loc_list(sudden_drop_df,'teamid',non_three_list)\n",
    "sudden_drop_list=getlist(sudden_drop_df,'teamid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter sudden drop - max\n",
    "\n",
    "drop_pct=0.9\n",
    "bounce=1\n",
    "\n",
    "#data=read_csv('filtered_data')\n",
    "data=fil_data\n",
    "name_dict=dict(zip(data.teamid,data.company))\n",
    "acc_list=getlist(data,'teamid')\n",
    "\n",
    "#1 for those accs with >= 4 months, identify drop based on first 3 months of data, put them into dicts\n",
    "drop_mon_dict=dict()\n",
    "drop_amount_dict=dict()\n",
    "rise_amount_dict=dict()\n",
    "total_month_dict=dict()\n",
    "first_drop_month=dict()\n",
    "max_month_dict=dict()\n",
    "zero_use=[]\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view_list=list(acc_df['function2'])\n",
    "    date_list=list(acc_df['date'])\n",
    "    max_view= max(view_list)\n",
    "    max_month=view_list.index(max_view)+1\n",
    "    if max_view != 0:\n",
    "        num_of_drop=0\n",
    "        total_months=len(view_list)\n",
    "        month_of_drop=[]\n",
    "        date_of_drop=[]\n",
    "        for i in range(len(view_list)-max_month):\n",
    "            month_view=view_list[i+max_month]\n",
    "            date=date_list[i+max_month]\n",
    "            change = (max_view-month_view)/max_view\n",
    "            if change >= drop_pct:\n",
    "                num_of_drop+=1\n",
    "                month_of_drop.append(i+max_month+1)\n",
    "                date_of_drop.append(date)\n",
    "        num_of_rise=total_months-num_of_drop-max_month\n",
    "        total_month_dict[acc]=total_months\n",
    "        drop_amount_dict[acc]=num_of_drop\n",
    "        rise_amount_dict[acc]=num_of_rise\n",
    "        max_month_dict[acc]=max_month\n",
    "        try:\n",
    "            first_drop_date[acc]=date_of_drop[0]\n",
    "        except:\n",
    "            first_drop_date[acc]=np.nan\n",
    "        try:\n",
    "            first_drop_month[acc]=month_of_drop[0]\n",
    "        except:\n",
    "            first_drop_month[acc]=np.nan        \n",
    "    else:\n",
    "        zero_use.append(acc)\n",
    "        \n",
    " #2 covert dicts to dataframe\n",
    "drop_df = pd.DataFrame(list(total_month_dict.items()),columns = ['team_id','total_months'])\n",
    "drop_mon=pd.DataFrame(list(drop_amount_dict.items()),columns = ['team_id','drop_mon'])\n",
    "rise_mon=pd.DataFrame(list(rise_amount_dict.items()),columns = ['team_id','rise_mon'])\n",
    "first_drop=pd.DataFrame(list(first_drop_month.items()),columns = ['team_id','first_drop_mon'])\n",
    "first_date=pd.DataFrame(list(first_drop_date.items()),columns = ['team_id','first_drop_date'])\n",
    "max_month=pd.DataFrame(list(max_month_dict.items()),columns = ['team_id','max_month'])\n",
    "drop_df=pd.merge(drop_df,drop_mon,on='team_id')\n",
    "drop_df=pd.merge(drop_df,rise_mon,on='team_id')\n",
    "drop_df=pd.merge(drop_df,first_drop,on='team_id')\n",
    "drop_df=pd.merge(drop_df,first_date,on='team_id')\n",
    "drop_df=pd.merge(drop_df,max_month,on='team_id')\n",
    "\n",
    "name_df=pd.DataFrame(list(name_dict.items()),columns = ['team_id','team_name'])\n",
    "drop_df=pd.merge(drop_df,name_df,on='team_id')\n",
    "drop_df=drop_df.replace('', 0) \n",
    "\n",
    "\n",
    "#3 filter those with more drop than rise, rising months of 1 or less\n",
    "drop_df['first_drop_mon'] = drop_df['first_drop_mon'].replace({0:np.nan})\n",
    "drop_df['after_rise_mon']=drop_df['rise_mon']-(drop_df['first_drop_mon']-drop_df['max_month']-1)\n",
    "\n",
    "sudden_drop_info=drop_df.loc[drop_df['drop_mon'] > drop_df['after_rise_mon']]\n",
    "sudden_drop_info=sudden_drop_info.loc[sudden_drop_info['after_rise_mon']<=bounce]\n",
    "\n",
    "sudden_drop_list=getlist(sudden_drop_info,'team_id')\n",
    "sudden_drop_df=loc_list(data,'teamid',drop_com_list)\n",
    "#save(sudden_drop_df,'sudden_drop_90_overlapped')\n",
    "#save(drop_df,'drop_info_90')\n",
    "\n",
    "\n",
    "#4 get common accounts from drop_df and low_use_df\n",
    "common_acc=list(set(drop_com_list)&set(consistent_low_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter sudden drop - with higher after_rise benchmark\n",
    "\n",
    "drop_pct=0.9\n",
    "bounce=1\n",
    "\n",
    "#data=read_csv('filtered_data')\n",
    "data=fil_data\n",
    "name_dict=dict(zip(data.teamid,data.company))\n",
    "acc_list=getlist(data,'teamid')\n",
    "#1 for those accs with >= 4 months, identify drop based on first 3 months of data, put them into dicts\n",
    "drop_mon_dict=dict()\n",
    "drop_amount_dict=dict()\n",
    "rise_amount_dict=dict()\n",
    "total_month_dict=dict()\n",
    "rebounce_dict=dict()\n",
    "first_drop_month=dict()\n",
    "first_drop_date=dict()\n",
    "max_month_dict=dict()\n",
    "zero_use=[]\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view_list=list(acc_df['function2'])\n",
    "    date_list=list(acc_df['date'])\n",
    "    max_view= max(view_list)\n",
    "    max_month=view_list.index(max_view)+1\n",
    "    drop_month=0\n",
    "    if max_view != 0:\n",
    "        num_of_drop=0\n",
    "        rebounce=0\n",
    "        total_months=len(view_list)\n",
    "        month_of_drop=[]\n",
    "        date_of_drop=[]\n",
    "        for i in range(len(view_list)-max_month):\n",
    "            month_view=view_list[i+max_month]\n",
    "            date=date_list[i+max_month]\n",
    "            change = (max_view-month_view)/max_view\n",
    "            if change >= drop_pct:\n",
    "                num_of_drop+=1\n",
    "                drop_month=i+max_month+1\n",
    "                month_of_drop.append(drop_month)\n",
    "                break\n",
    "        drop_num=drop_month\n",
    "        total_month_dict[acc]=total_months\n",
    "        drop_amount_dict[acc]=num_of_drop\n",
    "        for i in range(len(view_list)-drop_num):\n",
    "            month_view=view_list[i+drop_num]\n",
    "            if month_view > 0.5*max_view:\n",
    "                rebounce+=1\n",
    "        rebounce_dict[acc]=rebounce\n",
    "        max_month_dict[acc]=max_month\n",
    "        try:\n",
    "            first_drop_date[acc]=date_of_drop[0]\n",
    "        except:\n",
    "            first_drop_date[acc]=np.nan\n",
    "        try:\n",
    "            first_drop_month[acc]=month_of_drop[0]\n",
    "        except:\n",
    "            first_drop_month[acc]=np.nan        \n",
    "    else:\n",
    "        zero_use.append(acc)\n",
    "        \n",
    " #2 covert dicts to dataframe\n",
    "drop_df = pd.DataFrame(list(total_month_dict.items()),columns = ['team_id','total_months'])\n",
    "drop_mon=pd.DataFrame(list(drop_amount_dict.items()),columns = ['team_id','drop_mon'])\n",
    "rebounce=pd.DataFrame(list(rebounce_dict.items()),columns = ['team_id','rebounce'])\n",
    "first_drop=pd.DataFrame(list(first_drop_month.items()),columns = ['team_id','first_drop_mon'])\n",
    "first_date=pd.DataFrame(list(first_drop_date.items()),columns = ['team_id','first_drop_date'])\n",
    "max_month=pd.DataFrame(list(max_month_dict.items()),columns = ['team_id','max_month'])\n",
    "drop_df=pd.merge(drop_df,drop_mon,on='team_id')\n",
    "drop_df=pd.merge(drop_df,rebounce,on='team_id')\n",
    "drop_df=pd.merge(drop_df,first_drop,on='team_id')\n",
    "drop_df=pd.merge(drop_df,first_date,on='team_id')\n",
    "drop_df=pd.merge(drop_df,max_month,on='team_id')\n",
    "\n",
    "name_df=pd.DataFrame(list(name_dict.items()),columns = ['team_id','team_name'])\n",
    "drop_df=pd.merge(drop_df,name_df,on='team_id')\n",
    "drop_df=drop_df.replace('', 0) \n",
    "\n",
    "\n",
    "#3 filter those with rebounce less than 2\n",
    "test_drop_df=drop_df.loc[drop_df['rebounce'] <=2]\n",
    "test_drop_list=getlist(test_drop_df,'team_id')\n",
    "#sudden_drop_df=loc_list(data,'teamid',drop_com_list)\n",
    "#save(sudden_drop_df,'sudden_drop_90_overlapped')\n",
    "#save(drop_df,'drop_info_90')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter accounts with fluctuating use, ocassional drops in view\n",
    "#data=read_csv('filtered_data')\n",
    "data=fil_data\n",
    "name_dict=dict(zip(data.teamid,data.company))\n",
    "acc_list=getlist(data,'teamid')\n",
    "\n",
    "#1 get those with after rise month more than 1 and less than 50%\n",
    "df=drop_df\n",
    "com_list=getlist(df,'team_id')\n",
    "fluc_com_list=[]\n",
    "for com in com_list:\n",
    "    com_df=loc_one(df,'team_id',com)\n",
    "    total_mon=com_df['total_months'].values[0]\n",
    "    first_drop_mon=com_df['first_drop_mon'].values[0]\n",
    "    after_rise_mon=com_df['after_rise_mon'].values[0]\n",
    "    remaining_mon=total_mon-first_drop_mon+1\n",
    "    rise_num=round(remaining_mon/2)\n",
    "    if after_rise_mon>1 and after_rise_mon<=rise_num:\n",
    "        fluc_com_list.append(com)\n",
    "fluc_com_df=all_data.loc[all_data['teamid'].isin(fluc_com_list)]\n",
    "fluc_info_df=drop_df.loc[drop_df['team_id'].isin(fluc_com_list)]\n",
    "#save(fluc_info_df,'fluc_info_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyze r/s with churn\n",
    "#data=read_csv('filtered_data')\n",
    "data=fil_data\n",
    "name_dict=dict(zip(data.teamid,data.company))\n",
    "acc_list=getlist(data,'teamid')\n",
    "#1 read churn accounts \n",
    "churn_info=pd.read_csv(\"data.csv\",encoding='gb18030')\n",
    "churn_acc_list=list(churn_info['account'])\n",
    "churn_dict=dict()\n",
    "\n",
    "name_key_list = list(name_dict.keys()) \n",
    "name_val_list = list(name_dict.values()) \n",
    "churn_id_list=[]\n",
    "no_id_list=[]\n",
    "for acc in churn_acc_list:\n",
    "    try:\n",
    "        churn_id=name_key_list[name_val_list.index(acc)]\n",
    "        churn_id_list.append(churn_id)\n",
    "        churn_dict[acc]=churn_id\n",
    "    except:\n",
    "        no_id_list.append(acc)\n",
    "        \n",
    "#2 get pct of churned accounts in low_use accounts and sudden drop acc\n",
    "\n",
    "def get_cr(acc_list,name):\n",
    "    name=name\n",
    "    churn_acc=list(set(acc_list)&set(churn_id_list))\n",
    "    churn_rate=len(churn_acc)/len(acc_list)\n",
    "    print(name,':',len(acc_list),churn_rate)\n",
    "    \n",
    "get_cr(consistent_low_list,'consistent_low')\n",
    "get_cr(avg_low_list,'avg_low')\n",
    "get_cr(sudden_drop_list,'sudden_drop_maximum')\n",
    "get_cr(drop_com_list,'sudden_drop_initial')\n",
    "#get_cr(high_fluc_list,'fluc')\n",
    "get_cr(acc_list,'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get number of times view accrosses the avg line\n",
    "\n",
    "data=fil_data\n",
    "acc_list=getlist(data,'teamid')\n",
    "\n",
    "#1 for those accs with >= 4 months, identify drop based on first 3 months of data, put them into dicts\n",
    "drop_dict=dict()\n",
    "rise_dict=dict()\n",
    "drop_pct_dict=dict()\n",
    "rise_pct_dict=dict()\n",
    "change_pct_dict=dict()\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view_list=list(acc_df['function2'])\n",
    "    total_mon=len(view_list)\n",
    "    avg_view= sum(view_list)/len(view_list)\n",
    "    initial_view=view_list[0]\n",
    "    drop=0\n",
    "    rise=0\n",
    "    for i in range(len(view_list)-1):\n",
    "        prev_month_view=view_list[i]\n",
    "        after_month_view=view_list[i+1]\n",
    "        if prev_month_view < avg_view and after_month_view > avg_view:\n",
    "            rise+=1\n",
    "        elif prev_month_view > avg_view and after_month_view < avg_view :\n",
    "            drop+=1\n",
    "        drop_pct=drop/(total_mon-1)\n",
    "        rise_pct=rise/(total_mon-1)\n",
    "        change_pct=(drop+rise)/(total_mon-1)\n",
    "    drop_dict[acc]=drop\n",
    "    rise_dict[acc]=rise\n",
    "    drop_pct_dict[acc]=drop_pct\n",
    "    rise_pct_dict[acc]=rise_pct\n",
    "    change_pct_dict[acc]=change_pct\n",
    "    \n",
    "drop_num=pd.DataFrame(drop_dict.items(),columns=['teamid', 'drop_num'])\n",
    "rise_num=pd.DataFrame(rise_dict.items(),columns=['teamid', 'rise_num'])\n",
    "drop_pct=pd.DataFrame(drop_pct_dict.items(),columns=['teamid', 'drop_pct'])\n",
    "rise_pct=pd.DataFrame(rise_pct_dict.items(),columns=['teamid', 'rise_pct'])\n",
    "change_pct=pd.DataFrame(change_pct_dict.items(),columns=['teamid', 'change_pct'])\n",
    "\n",
    "fluc_df=pd.merge(drop_num,rise_num,on='teamid')\n",
    "fluc_df=pd.merge(fluc_df,drop_pct,on='teamid')\n",
    "fluc_df=pd.merge(fluc_df,rise_pct,on='teamid')\n",
    "fluc_df=pd.merge(fluc_df,change_pct,on='teamid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter number of month/pct of month under the low usage line\n",
    "data=fil_data\n",
    "acc_list=getlist(data,'teamid')\n",
    "\n",
    "#1 for those accs with >= 4 months, identify drop based on first 3 months of data, put them into dicts\n",
    "low_dict=dict()\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view_list=list(acc_df['function2'])\n",
    "    total_mon=len(view_list)\n",
    "    low_mon=0\n",
    "    try:\n",
    "        users=user_num_dict[acc]\n",
    "        for i in range(len(view_list)):\n",
    "            month_view=view_list[i]\n",
    "            if month_view < low_view*users:\n",
    "                low_mon+=1\n",
    "        low_mon_pct=low_mon/total_mon\n",
    "        low_dict[acc]=low_mon_pct\n",
    "    except:\n",
    "        print(acc)\n",
    "low_mon_pct=pd.DataFrame(low_dict.items(),columns=['teamid', 'low_mon_pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop (y/n) in first 3 and last 3 months\n",
    "data=fil_data\n",
    "acc_list=getlist(data,'teamid')\n",
    "first_drop_dict=dict()\n",
    "last_drop_dict=dict()\n",
    "\n",
    "\n",
    "for acc in acc_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    view_list=list(acc_df['function2'])\n",
    "    total_mon=len(view_list)\n",
    "    first_3=view_list[0:3]\n",
    "    first_avg=sum(first_3)/3\n",
    "    last_3=view_list[(total_mon-3):(total_mon+1)]\n",
    "    last_avg=sum(last_3)/3\n",
    "    first_drop=0\n",
    "    last_drop=0\n",
    "    for i in range(2):\n",
    "        prev_month_view=first_3[i]\n",
    "        after_month_view=first_3[i+1]\n",
    "        if prev_month_view > first_avg and after_month_view < first_avg:\n",
    "            first_drop+=1\n",
    "    for i in range(2):\n",
    "        prev_month_view=last_3[i]\n",
    "        after_month_view=last_3[i+1]\n",
    "        if prev_month_view > last_avg and after_month_view < last_avg:\n",
    "            last_drop+=1\n",
    "    first_drop_dict[acc]=first_drop\n",
    "    last_drop_dict[acc]=last_drop\n",
    "    \n",
    "first_drop_num=pd.DataFrame(first_drop_dict.items(),columns=['teamid', 'first_drop'])\n",
    "last_drop_num=pd.DataFrame(last_drop_dict.items(),columns=['teamid', 'last_drop'])\n",
    "trend_df=pd.merge(first_drop_num,last_drop_num,on='teamid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### qrate for all the accounts\n",
    "data=fil_data\n",
    "data['q_rate']=data['total_qualified']/data['function2']\n",
    "q_rate=data.groupby(['teamid','company'])['q_rate'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlog(df,col1,log_col):\n",
    "    zero_list=getlist(loc_one(df,col1,0),'teamid')\n",
    "    non_zero_set=set(getlist(df,'teamid'))-set(zero_list)\n",
    "    df=loc_list(df, 'teamid',non_zero_set)\n",
    "    df2=df.copy(deep=True)\n",
    "    df2[log_col]=np.log10(df[col1])\n",
    "    return df2\n",
    "#avg_view=getlog(avg_view,'avg_user_view','log_avg_view')\n",
    "#avg_view2['log_avg_view']=np.log10(avg_view2['avg_user_view'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "#raw_data=read_csv('training_data')\n",
    "raw_data=fil_data\n",
    "train_data=pd.DataFrame()\n",
    "team_id=getlist(raw_data,'teamid')\n",
    "train_data['teamid']=team_id\n",
    "\n",
    "#3.average view\n",
    "avg_view=raw_data.groupby('teamid')['function2'].mean().reset_index().rename(columns={'function2':'avg_view'})\n",
    "avg_view=getlog(avg_view,'avg_view','log_avg_view')\n",
    "train_data=pd.merge(train_data,avg_view[['teamid','log_avg_view']],on='teamid')\n",
    "\n",
    "\n",
    "# s.d. of view\n",
    "std_view=raw_data.groupby('teamid')['function2'].std().reset_index().rename(columns={'function2':'std'})\n",
    "std_view=getlog(std_view,'std','log_std')\n",
    "train_data=pd.merge(train_data,std_view[['teamid','log_std']],on='teamid')\n",
    "\n",
    "\n",
    "#1. first month of usage \n",
    "#first_mon=raw_data.groupby('teamid')['function2'].first().reset_index().rename(columns={'function2':'first_month'})\n",
    "#first_mon=getlog(first_mon,'first_month','log_first_month')\n",
    "#train_data=pd.merge(train_data,first_mon[['teamid','first_usage']],on='teamid')\n",
    "#train_data=pd.merge(train_data,first_mon[['teamid','log_first_month']],on='teamid')\n",
    "\n",
    "#2.last 1 month of usage \n",
    "last_mon=raw_data.groupby('teamid')['function2'].last().reset_index().rename(columns={'function2':'last_month'})\n",
    "last_mon=getlog(last_mon,'last_month','log_last_month')\n",
    "train_data=pd.merge(train_data,last_mon[['teamid','log_last_month']],on='teamid')\n",
    "\n",
    "#4.industry 5.segments \n",
    "segment_info=read_csv('teamid with names')\n",
    "smb_list=getlist(loc_one(segment_info,'segment','SMB'),'teamid')\n",
    "mm_list=getlist(loc_one(segment_info,'segment','MM'),'teamid')\n",
    "ent_list=getlist(loc_one(segment_info,'segment','ENT'),'teamid')\n",
    "train_data['SMB']=0\n",
    "train_data['MM']=0\n",
    "train_data.loc[train_data['teamid'].isin(smb_list),'SMB']=1\n",
    "train_data.loc[train_data['teamid'].isin(mm_list),'MM']=1\n",
    "\n",
    "#6 pct of times it crosses the the avg line(fluctuation pattern)\n",
    "train_data=pd.merge(train_data,fluc_df[['teamid','drop_num','rise_num','change_pct']],on='teamid')\n",
    "\n",
    "#7.whether there is a sudden dorp\n",
    "train_data['sudden_drop']=0\n",
    "train_data.loc[train_data['teamid'].isin(sudden_drop_list),'sudden_drop']=1\n",
    "\n",
    "#8.number of month/pct of month under the low usage line\n",
    "train_data=pd.merge(train_data,low_mon_pct,on='teamid')\n",
    "\n",
    "#9 number of users\n",
    "user_num=pd.DataFrame(user_num_dict.items(),columns=['teamid', 'num_of_users'])\n",
    "user_num['log_num_of_users']=np.log10(user_num['num_of_users'])\n",
    "train_data=pd.merge(train_data,user_num[['teamid','log_num_of_users']],on='teamid')\n",
    "\n",
    "# drop trend in first few and last few months\n",
    "#train_data=pd.merge(train_data,trend_df,on='teamid')\n",
    "\n",
    "#10.number of months \n",
    "#num_of_mon=raw_data.groupby('teamid')['function2'].count().reset_index().rename(columns={'function2':'num_of_mon'})\n",
    "#train_data=pd.merge(train_data,num_of_mon,on='teamid')\n",
    "\n",
    "#11 q_rate \n",
    "train_data=pd.merge(train_data,q_rate[['teamid','q_rate']],on='teamid')\n",
    "\n",
    "\n",
    "#12 whethter it has churned\n",
    "train_data['churned']='non_churned'\n",
    "train_data.loc[train_data['teamid'].isin(churn_id_list),'churned']='churned'\n",
    "\n",
    "train_data=train_data.dropna()\n",
    "#save(train_data,'train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(train_data.columns)\n",
    "#split a validation dataset\n",
    "array=train_data.values\n",
    "num=total-1\n",
    "X=array[:,1:num]\n",
    "y=array[:,num]\n",
    "X_train,X_validation,Y_train,Y_validation=train_test_split(X,y,test_size=0.20,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols=list(train_data.columns)\n",
    "cols.remove('SMB')\n",
    "cols.remove('MM')\n",
    "cols.remove('sudden_drop')\n",
    "for i in range(len(cols)-2):\n",
    "    col=cols[i+1]\n",
    "    plot_df=train_data[['teamid',col]]\n",
    "    #plot_df.plot(kind='box', subplots=True, layout=(1,1), sharex=False, sharey=False)\n",
    "    #plot_df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model and check accuracy \n",
    "model = (LogisticRegression(solver='liblinear', multi_class='ovr'))\n",
    "kfold=StratifiedKFold(n_splits=5,random_state=1,shuffle=True)\n",
    "cv_result=cross_val_score(model,X_train,Y_train,cv=kfold,scoring='accuracy')\n",
    "#print(cv_result.mean(), cv_result.std())\n",
    "\n",
    "#make prediction\n",
    "model.fit(X_train,Y_train)\n",
    "predictions = model.predict(X_validation)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_,model.intercept_)\n",
    "importance = model.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with RFE\n",
    "from sklearn.feature_selection import RFE\\\n",
    "# feature extraction\n",
    "model = (LogisticRegression(solver='liblinear', multi_class='ovr'))\n",
    "rfe = RFE(model, 5)\n",
    "fit = rfe.fit(X_train,Y_train)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "rfe.fit(X_train,Y_train)\n",
    "predictions = rfe.predict(X_validation)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "raw_data=read_csv('training_data')\n",
    "train_data=pd.DataFrame()\n",
    "team_id=getlist(raw_data,'teamid')\n",
    "train_data['teamid']=team_id\n",
    "\n",
    "# average view\n",
    "avg_view=raw_data.groupby('teamid')['function2'].mean().reset_index().rename(columns={'function2':'avg_view'})\n",
    "avg_view=getlog(avg_view,'avg_view','log_avg_view')\n",
    "train_data=pd.merge(train_data,avg_view2[['teamid','log_avg_view']],on='teamid')\n",
    "\n",
    "\n",
    "# s.d. of view\n",
    "std_view=raw_data.groupby('teamid')['function2'].std().reset_index().rename(columns={'function2':'std'})\n",
    "std_view=getlog(std_view,'std','log_std')\n",
    "train_data=pd.merge(train_data,std_view[['teamid','log_std']],on='teamid')\n",
    "\n",
    "\n",
    "#1. first month of usage \n",
    "first_mon=raw_data.groupby('teamid')['function2'].first().reset_index().rename(columns={'function2':'first_month'})\n",
    "first_mon=getlog(first_mon,'first_month','log_first_month')\n",
    "#train_data=pd.merge(train_data,first_mon[['teamid','first_usage']],on='teamid')\n",
    "train_data=pd.merge(train_data,first_mon[['teamid','log_first_month']],on='teamid')\n",
    "\n",
    "#2.last 1 month of usage \n",
    "#last_mon=raw_data.groupby('teamid')['function2'].last().reset_index().rename(columns={'function2':'last_month'})\n",
    "#last_mon=getlog(last_mon,'last_month','log_last_month')\n",
    "#train_data=pd.merge(train_data,last_mon[['teamid','log_last_month']],on='teamid')\n",
    "\n",
    "#4.industry 5.segments \n",
    "segment_info=read_csv('teamid with names')\n",
    "smb_list=getlist(loc_one(segment_info,'segment','SMB'),'teamid')\n",
    "mm_list=getlist(loc_one(segment_info,'segment','MM'),'teamid')\n",
    "ent_list=getlist(loc_one(segment_info,'segment','ENT'),'teamid')\n",
    "train_data['SMB']=0\n",
    "train_data['MM']=0\n",
    "train_data.loc[train_data['teamid'].isin(smb_list),'SMB']=1\n",
    "train_data.loc[train_data['teamid'].isin(mm_list),'MM']=1\n",
    "\n",
    "#6 number of times it crosses the the avg line(fluctuation pattern)\n",
    "train_data=pd.merge(train_data,fluc_df[['teamid','drop_num','rise_num']],on='teamid')\n",
    "\n",
    "#7.whether there is a sudden dorp\n",
    "train_data['sudden_drop']=0\n",
    "train_data.loc[train_data['teamid'].isin(sudden_drop_list),'sudden_drop']=1\n",
    "\n",
    "#8.number of month/pct of month under the low usage line\n",
    "train_data=pd.merge(train_data,low_mon_pct,on='teamid')\n",
    "\n",
    "#9 number of users\n",
    "user_num=pd.DataFrame(user_num_dict.items(),columns=['teamid', 'num_of_users'])\n",
    "user_num['log_num_of_users']=np.log10(user_num['num_of_users'])\n",
    "train_data=pd.merge(train_data,user_num[['teamid','log_num_of_users']],on='teamid')\n",
    "\n",
    "\n",
    "#10.number of months \n",
    "#num_of_mon=raw_data.groupby('teamid')['function2'].count().reset_index().rename(columns={'function2':'num_of_mon'})\n",
    "#train_data=pd.merge(train_data,num_of_mon,on='teamid')\n",
    "\n",
    "#11 whethter it has churned\n",
    "train_data['churned']='non_churned'\n",
    "train_data.loc[train_data['teamid'].isin(churn_id_list),'churned']='churned'\n",
    "\n",
    "train_data=train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#淘汰的Model\n",
    "raw_data=read_csv('training_data')\n",
    "train_data=pd.DataFrame()\n",
    "team_id=getlist(raw_data,'teamid')\n",
    "train_data['teamid']=team_id\n",
    "\n",
    "#3.average view\n",
    "avg_view=raw_data.groupby('teamid')['function2'].mean().reset_index().rename(columns={'function2':'avg_view'})\n",
    "user_num=pd.DataFrame(user_num_dict.items(),columns=['teamid', 'num_of_users'])\n",
    "avg_view=pd.merge(avg_view,user_num,on='teamid')\n",
    "avg_view['avg_user_view']=avg_view['avg_view']/avg_view['num_of_users']\n",
    "\n",
    "avg_view=getlog(avg_view,'avg_user_view','log_avg_view')\n",
    "train_data=pd.merge(train_data,avg_view[['teamid','log_avg_view']],on='teamid')\n",
    "\n",
    "# s.d. of view\n",
    "#std_view=raw_data.groupby('teamid')['function2'].std().reset_index().rename(columns={'function2':'std'})\n",
    "#train_data=pd.merge(train_data,std_view[['teamid','std']],on='teamid')\n",
    "\n",
    "\n",
    "#1. first month of usage \n",
    "first_mon=raw_data.groupby('teamid')['function2'].first().reset_index().rename(columns={'function2':'first_month'})\n",
    "#first_mon=pd.merge(first_mon,user_num,on='teamid')\n",
    "first_mon=pd.merge(first_mon,avg_view,on='teamid')\n",
    "first_mon['avg_first_view']=first_mon['first_month']/first_mon['num_of_users']\n",
    "#first_mon['first_usage']=first_mon['avg_first_view']/first_mon['avg_user_view']\n",
    "first_mon=getlog(first_mon,'avg_first_view','log_first_view')\n",
    "\n",
    "#train_data=pd.merge(train_data,first_mon[['teamid','first_usage']],on='teamid')\n",
    "train_data=pd.merge(train_data,first_mon[['teamid','log_first_view']],on='teamid')\n",
    "\n",
    "#2.last 1 month of usage \n",
    "last_mon=raw_data.groupby('teamid')['function2'].last().reset_index().rename(columns={'function2':'last_month'})\n",
    "last_mon=pd.merge(last_mon,avg_view,on='teamid')\n",
    "last_mon['avg_last_view']=last_mon['last_month']/last_mon['num_of_users']\n",
    "#last_mon['last_usage']=last_mon['avg_last_view']/last_mon['avg_user_view']\n",
    "last_mon=getlog(last_mon,'avg_last_view','log_last_view')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PLOT GRAPHS\n",
    "#1 split into companies\n",
    "def plotlines(df):\n",
    "    team_id=getlist(df,'teamid')\n",
    "    for i in range(len(team_id)):\n",
    "        comid=team_id[i]\n",
    "        com_name=name_dict[comid]\n",
    "        try:\n",
    "            user_num=user_num_dict[comid]\n",
    "            data=loc_one(df,'teamid',comid)\n",
    "            graph_data=data[['date','function2']]\n",
    "            graph_data=graph_data.set_index('date')\n",
    "            print(com_name,\":\",user_num,\"users\")\n",
    "            graph_data.plot()\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1),loc='upper left', borderaxespad=0.)\n",
    "            plt.title(com_name,fontsize=24)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(user_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_churn(acc_list):\n",
    "    churn_acc=list(set(acc_list)&set(churn_id_list))\n",
    "    churn_df=loc_list(fil_data,'teamid',churn_acc)\n",
    "    plotlines(churn_df)\n",
    "\n",
    "def plot_non_churn(acc_list):\n",
    "    churn_acc=list(set(acc_list)-set(churn_id_list))\n",
    "    churn_df=loc_list(data,'teamid',churn_acc)\n",
    "    plotlines(churn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cr(consistent_low_list,'consistent_low')\n",
    "get_cr(avg_low_list,'avg_low')\n",
    "get_cr(sudden_drop_list,'sudden_drop_maximum')\n",
    "get_cr(drop_com_list,'sudden_drop_initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 graphs for churned\n",
    "plot_data=read_csv('filtered_data8')\n",
    "churn_df=loc_list(plot_data,'teamid',churn_id_list)\n",
    "churn_list=getlist(churn_df,'teamid')\n",
    "test_list=list(set(churn_list)-set(consistent_low_list)-set(avg_low_list)-set(sudden_drop_list)-set(drop_com_list))\n",
    "test_df=loc_list(plot_data,'teamid',test_list)\n",
    "plotlines(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_others=list(set(churn_id_list)-set(sudden_drop_list))\n",
    "churn_others_df=loc_list(data,'teamid',churn_others)\n",
    "plotlines(churn_others_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyze sudden drop pattern & fluc pattern\n",
    "\n",
    "sudden_drop_info=drop_df.loc[drop_df['team_id'].isin(drop_com_list)]\n",
    "fluc_info=drop_df.loc[drop_df['team_id'].isin(fluc_com_list)]\n",
    "\n",
    "df=fluc_info\n",
    "\n",
    "#1 check if there is a common trend in the number of month of drop\n",
    "drop_month_grp=sudden_drop_info.groupby('first_drop_mon')['team_name'].count().reset_index()\n",
    "\n",
    "#2 check if there is a common trend in the actual month of drop\n",
    "drop_date_grp=sudden_drop_info.groupby('first_drop_date')['team_name'].count().reset_index()\n",
    "\n",
    "#3 check id there is common trend in after rise month\n",
    "after_rise_grp=sudden_drop_info.groupby('after_rise_mon')['team_name'].count().reset_index()\n",
    "\n",
    "#3 plot graph\n",
    "df=sudden_drop_df[['date','teamid','function2']]\n",
    "df = df.pivot(index='date', columns='teamid', values='function2')\n",
    "#df.plot()\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1),loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 analyse churn rate for different drop month\n",
    "df=drop_month_grp\n",
    "col='first_drop_mon'\n",
    "for i in range(len(df[col])):\n",
    "    num=df[col][i]\n",
    "    four_month_df=sudden_drop_info.loc[sudden_drop_info[col]==num]\n",
    "    four_month=four_month_df['team_id']\n",
    "    test=sudden_drop_df.loc[sudden_drop_df['teamid'].isin(four_month)]\n",
    "    #save(test,'test')\n",
    "    print(num)\n",
    "    try:\n",
    "        get_cr(test)\n",
    "    except:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 deep dive for churn reason, combine all info into 1 df\n",
    "churn_drop_df=sudden_drop_info.loc[sudden_drop_info['team_id'].isin(churn_drop)]\n",
    "churn_drop_df=churn_drop_df.rename(columns={'team_id':'teamid'})\n",
    "churn_drop_info=pd.merge(churn_drop_df,churn_df,on='teamid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=read_csv('month_all')\n",
    "### THESE ARE MANUAL WORK \n",
    "#identify possible debook accounts and accounts with low months\n",
    "\n",
    "#1 clean churn data,filter those which may be debook accounts - filter those with usage less than 12 months\n",
    "churn_info=pd.read_csv(\"data2.csv\",encoding='gb18030')\n",
    "churn_acc_list=getlist(churn_info,'account')\n",
    "\n",
    "churn_df=loc_list(all_data,'company',churn_acc_list)\n",
    "churn_count=churn_df.groupby('company')['function2'].count().reset_index()\n",
    "#save(churn_count,'churn_test')\n",
    "\n",
    "#2 filter those with months less than 12\n",
    "churn_count=churn_count.loc[churn_count['function2']<12]\n",
    "#conclusion: none of the churn accounts are debook\n",
    "\n",
    "#3.1 clean accounts with missing month data, that are non churned and non aug account\n",
    "all_acc_list=list(all_data['company'].drop_duplicates())\n",
    "all_acc_id=list(all_data['teamid'].drop_duplicates())\n",
    "non_churn_list=list(set(all_acc_list)-set(churn_acc_list))\n",
    "data=all_data.loc[all_data['company'].isin(non_churn_list)]\n",
    "\n",
    "last_date=data.groupby('teamid')['date'].last().reset_index()\n",
    "aug_df=last_date.loc[last_date['date']== '2020/8/31']\n",
    "aug_list=list(aug_df['teamid'].drop_duplicates())\n",
    "non_aug_list=list(set(all_acc_id)-set(aug_list))\n",
    "data=data.loc[data['teamid'].isin(non_aug_list)]\n",
    "\n",
    "#3.2 clean new accounts \n",
    "data_count=fil_data.groupby('teamid')['function2'].count().reset_index()\n",
    "data_missing=data_count.loc[data_count['function2']<13]\n",
    "missing_acc_list=list(data_missing['teamid'].drop_duplicates())\n",
    "data_low_month=data.loc[data['teamid'].isin(missing_acc_list)]\n",
    "\n",
    "###filter accs with records of only a few months \n",
    "\n",
    "def filtermons(df,num,with_aug):\n",
    "#1 filter those with end date in aug as they are not applicable\n",
    "    all_acc=getlist(df,'teamid')\n",
    "    last_date=df.groupby('teamid')['date'].last().reset_index()\n",
    "    aug_df=loc_one(last_date,'date','2020/8/31')\n",
    "    aug_list=getlist(aug_df,'teamid')\n",
    "    if with_aug == 'without_aug':\n",
    "        filter_list=list(set(all_acc)-set(aug_list))\n",
    "    elif with_aug == 'with_aug':\n",
    "        filter_list=all_acc\n",
    "    elif with_aug == 'only_aug':\n",
    "        filter_list=aug_list\n",
    "\n",
    "#2 count the number of months for filtered accounts\n",
    "    data=loc_list(df,'teamid',filter_list)\n",
    "    num_of_mon=data.groupby('teamid')['date'].count().reset_index()\n",
    "    low_mon=num_of_mon.loc[num_of_mon['date'] < num]\n",
    "    low_mon_acc=getlist(low_mon,'teamid')\n",
    "\n",
    "#3 delete these accounts from all_data\n",
    "    normal_mon_acc=list(set(all_acc)-set(low_mon_acc))\n",
    "    return normal_mon_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning\n",
    "#1 remove debook and other problematic accounts\n",
    "all_data=read_csv('month_all')\n",
    "all_acc_list=getlist(all_data,'teamid')\n",
    "name_dict=dict(zip(all_data.teamid,all_data.company))\n",
    "del_acc_df=read_excel('problematic_acc','others')\n",
    "del_acc_list=getlist(del_acc_df,'teamid')\n",
    "non_del_list=list(set(all_acc_list)-set(del_acc_list))\n",
    "data=loc_list(all_data,'teamid',non_del_list)\n",
    "data=data.copy(deep=True)\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "\n",
    "\n",
    "#2.1 fill in accounts with missing month data: read the service start date and end date\n",
    "service_date_df=read_excel('service_term','without_debook')\n",
    "service_date_df['Service Start date']=pd.to_datetime(service_date_df['Service Start date'])\n",
    "service_date_df['Service End Date']=pd.to_datetime(service_date_df['Service End Date'])\n",
    "\n",
    "earlist_date_df=service_date_df.groupby(['account','BMC ID'])['Service Start date'].min().reset_index()\n",
    "lastest_date_df=service_date_df.groupby(['account','BMC ID'])['Service End Date'].max().reset_index()\n",
    "date_range_df=pd.merge(earlist_date_df,lastest_date_df,on=['account','BMC ID'])\n",
    "#adjust the start and end to jan 2019 and aug 2020\n",
    "start_date=datetime(2019, 1, 1)\n",
    "condition1=date_range_df['Service Start date']<start_date\n",
    "date_range_df.loc[condition1, 'Service Start date'] = start_date\n",
    "end_date=datetime(2020, 8, 31)\n",
    "condition2=date_range_df['Service End Date']>end_date\n",
    "date_range_df.loc[condition2, 'Service End Date'] = end_date\n",
    "\n",
    "\n",
    "#2.2 compare the accounts, fill in those with missing months\n",
    "fil_data=pd.DataFrame()\n",
    "acc_id_list=getlist(data,'teamid')\n",
    "unfound_acc_list=list()\n",
    "for acc in acc_id_list:\n",
    "    acc_df=loc_one(data,'teamid',acc)\n",
    "    try:\n",
    "        date_df=loc_one(date_range_df,'BMC ID',acc)\n",
    "        #first_date=list(acc_df['date'])[0]\n",
    "        start_date=list(date_df['Service Start date'])[0]\n",
    "        end_date=list(date_df['Service End Date'])[0]\n",
    "        r=pd.date_range(start=start_date,end=end_date,freq='M')\n",
    "        acc_df=acc_df.set_index('date').reindex(r).fillna(0.0).rename_axis('date').reset_index()\n",
    "        acc_df.loc[:,'teamid']=acc\n",
    "        acc_df.loc[:,'team_subscription_poc_company']=name_dict[acc]\n",
    "    except:\n",
    "        unfound_acc_list.append(acc)\n",
    "        acc_df=acc_df.reset_index()\n",
    "    fil_data=fil_data.append(acc_df,sort=True)\n",
    "    \n",
    "#3 filter out the accounts with usage data less than 3 months \n",
    "#acc_list1=filtermons(fil_data,6,'with_aug')\n",
    "#acc_list2=filtermons(fil_data,12,'only_aug')\n",
    "#normal_acc_list=list(set(acc_list1)&set(acc_list2))\n",
    "#fil_data=loc_list(fil_data,'teamid',normal_acc_list)\n",
    "\n",
    "\n",
    "#4 clean new accounts\n",
    "last_date=fil_data.groupby('teamid')['date'].last().reset_index()\n",
    "aug_list=getlist(last_date.loc[last_date['date']== '2020/8/31'],'teamid')\n",
    "data_count=loc_list(fil_data,'teamid',aug_list).groupby('teamid')['function2'].count().reset_index()\n",
    "data_new=data_count.loc[data_count['function2']<13]\n",
    "new_acc_list=getlist(data_new,'teamid')\n",
    "all_acc_list=getlist(fil_data,'teamid')\n",
    "old_acc_list=set(all_acc_list)-set(new_acc_list)\n",
    "fil_data=fil_data.loc[fil_data['teamid'].isin(old_acc_list)]\n",
    "fil_acc_list=getlist(fil_data,'teamid')\n",
    "\n",
    "\n",
    "#5 clean accounts with less than 12 months \n",
    "full_acc_list=filtermons(fil_data,12,'with_aug')\n",
    "fil_data=loc_list(fil_data,'teamid',full_acc_list)\n",
    "\n",
    "fil_acc_list=getlist(fil_data,'teamid')\n",
    "save(fil_data,'filtered_data8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
